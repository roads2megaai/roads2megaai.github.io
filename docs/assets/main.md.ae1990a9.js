import{_ as e,c as a,o as r,N as i}from"./chunks/framework.0799945b.js";const t="/assets/Acun_Bilge.437304a6.jpg",n="/assets/frankle.77743962.png",o="/assets/furonghuang.66ee4abe.jpg",s="/assets/luochen_2.32791de7.jpg",l="/assets/mitzenmacher_michael2.13dba04d.webp",_=JSON.parse('{"title":"Research On Algorithms & Data Structures (ROADS) to Mega-AI Models Workshop","description":"","frontmatter":{"layout":"doc"},"headers":[],"relativePath":"main.md"}'),h={name:"main.md"},c=i('<h1 id="research-on-algorithms-data-structures-roads-to-mega-ai-models-workshop" tabindex="-1">Research On Algorithms &amp; Data Structures (ROADS) to Mega-AI Models Workshop <a class="header-anchor" href="#research-on-algorithms-data-structures-roads-to-mega-ai-models-workshop" aria-label="Permalink to &quot;Research On Algorithms &amp; Data Structures (ROADS) to Mega-AI Models Workshop&quot;">​</a></h1><h2 id="workshop-summary" tabindex="-1">Workshop Summary <a class="header-anchor" href="#workshop-summary" aria-label="Permalink to &quot;Workshop Summary&quot;">​</a></h2><p>The state-of-the-art on numerous machine learning (ML) benchmarks comes from training enormous neural network models on expensive, specialized hardware with massive quantities of data. However, this route to success in deep learning is unsustainable. Training a large transformer model in natural language processing, for instance, can incur a higher carbon footprint than the <a href="https://www.technologyreview.com/2019/06/06/239031/training-a-single-ai-model-can-emit-as-much-carbon-as-five-cars-in-their-lifetimes/" target="_blank" rel="noreferrer">total lifetime cost of five cars</a>. In addition, these massive models require immense memory and computing resources during deployment, which hinders their practical impact. To realize the full promise and benefits of artificial intelligence, we must solve these scalability challenges prevalent in both training and inference and design new algorithms with step-function improvements in efficiency. This workshop aims to bring together both computer science researchers and practitioners focused on ML efficiency to offer innovative solutions towards efficient modeling workflows grounded in principled algorithm design.</p><h2 id="keynote-speakers" tabindex="-1">Keynote Speakers <a class="header-anchor" href="#keynote-speakers" aria-label="Permalink to &quot;Keynote Speakers&quot;">​</a></h2><img src="'+t+'" style="width:16em;"><ul><li><a href="https://bilgeacun.github.io/" target="_blank" rel="noreferrer">Bilge Acun</a> (Meta, USA)</li></ul><img src="'+n+'" style="width:16em;"><ul><li><a href="http://www.jfrankle.com/" target="_blank" rel="noreferrer">Jonathan Frankle</a> (Harvard University/Mosaic ML, USA)</li></ul><img src="'+o+'" style="width:16em;"><ul><li><a href="http://furong-huang.com/" target="_blank" rel="noreferrer">Furong Huang</a> (University of Maryland, USA)</li></ul><img src="'+s+'" style="width:16em;"><ul><li><a href="http://chen-luo.com/" target="_blank" rel="noreferrer">Chen Luo</a> (Amazon)</li></ul><img src="'+l+'" style="width:16em;"><ul><li><a href="https://www.eecs.harvard.edu/~michaelm/" target="_blank" rel="noreferrer">Michael Mitzenmacher</a> (Harvard University, USA)</li></ul><h2 id="schedule-thursday-june-8" tabindex="-1">Schedule (Thursday, June 8) <a class="header-anchor" href="#schedule-thursday-june-8" aria-label="Permalink to &quot;Schedule (Thursday, June 8)&quot;">​</a></h2><ul><li>8:30am-8:45am Welcome and Opening Remarks (Workshop Organizers)</li><li>8:45am-9:45am Keynote #1: Prof. Michael Mitzenmacher (Harvard)</li><li>9:45am-10:10am Invited Talk</li><li>10:10am-10:35am Invited Talk</li><li>10:35am-10:50am Break</li><li>10:50am-11:35am Keynote #2: Dr. Bilge Acun (Meta)</li><li>11:35-12:00pm Invited Talk</li><li>12:00pm-12:25pm Invited Talk</li><li>12:25pm-1:30pm Lunch Break</li><li>1:30pm-2:15pm Keynote #3: Prof. Jonathan Frankle (Harvard/Mosaic ML)</li><li>2:15pm-2:45pm Keynote #4: Prof. Furong Huang (UMD)</li><li>2:45pm-3:30pm Keynote #5: Dr. Chen Luo (Amazon)</li><li>3:30pm-3:45pm Break</li><li>3:45pm-4:30pm Panel Discussion (Bilge Acun, Jonathan Frankle, Chen Luo, Michael Mitzenmacher, Anshumali Shrivastava)</li><li>4:30pm - Social</li></ul><h2 id="organizing-committee" tabindex="-1">Organizing Committee <a class="header-anchor" href="#organizing-committee" aria-label="Permalink to &quot;Organizing Committee&quot;">​</a></h2><ul><li><a href="https://ottovonxu.github.io/" target="_blank" rel="noreferrer">Zhaozhuo Xu</a> (Rice University, USA)</li><li><a href="https://www.linkedin.com/in/aditya-desai-8078811a" target="_blank" rel="noreferrer">Aditya Desai</a> (Rice University, USA)</li><li><a href="https://vihan-lakshman.github.io/" target="_blank" rel="noreferrer">Vihan Lakshman</a> (ThirdAI, USA)</li><li><a href="https://www.cs.rice.edu/~as143/" target="_blank" rel="noreferrer">Anshumali Shrivastava</a> (Rice University/ThirdAI, USA)</li></ul><h2 id="call-for-papers" tabindex="-1">Call for Papers <a class="header-anchor" href="#call-for-papers" aria-label="Permalink to &quot;Call for Papers&quot;">​</a></h2><p>This workshop encourages submissions on original research, benchmarks, in-progress research results, and position papers. We also invite submissions of previously accepted papers where authors had limited presentation opportunities (e.g. due to pandemic-related constraints). We do not have strict requirements on paper lengths, but encourage authors to adhere to a maximum of <strong>10-pages</strong> (excluding references) in MLSys 2023 format. We will manage submissions through OpenReview, but the review-process will not be open.</p><h2 id="scope" tabindex="-1">Scope <a class="header-anchor" href="#scope" aria-label="Permalink to &quot;Scope&quot;">​</a></h2><p>The technical topics of interest at this workshop include (but are not limited to):</p><ul><li>Algorithms and data structures to improve the computational efficiency of neural network training and inference</li><li>Algorithmic solutions for deploying machine learning models on resource-constrained devices</li><li>Model compression approaches for training and inference, including pruning, quantization, and parameter sharing</li><li>Data reduction techniques (e.g. sketching, sampling, coresets) for efficient training and inference</li><li>Algorithmic techniques to enable longer sequence language models, higher resolution images for vision models, wide and deep neural networks, and other model architectures of interest to the community</li></ul><h2 id="submission" tabindex="-1">Submission <a class="header-anchor" href="#submission" aria-label="Permalink to &quot;Submission&quot;">​</a></h2><p><a href="https://openreview.net/group?id=MLSys.org/2023/Workshop/ROADS" target="_blank" rel="noreferrer">OpenReview Link</a></p><p>Please format submissions using the MLSys 2023 style files.</p><p>The reviewing process will be double-blind. Please submit anonymized papers that omit all information about author identities. There are no formal proceedings generated from this workshop. Accepted papers will be made available on OpenReview.</p><h2 id="contact" tabindex="-1">Contact <a class="header-anchor" href="#contact" aria-label="Permalink to &quot;Contact&quot;">​</a></h2><p>Contact the organizers: <a href="mailto:zx22@rice.edu" target="_blank" rel="noreferrer">zx22@rice.edu</a></p>',29),m=[c];function u(d,p,g,f,k,b){return r(),a("div",null,m)}const v=e(h,[["render",u]]);export{_ as __pageData,v as default};
